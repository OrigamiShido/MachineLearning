{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 使用AdaBoost 元算法进行病马死亡率的预测\n",
    "使用horseColicTraining2.txt 文件作为训练集，\n",
    "horseColicTest2.txt 文件作为测试集，使用基于单层决策树的\n",
    "AdaBoost 算法（弱分类器数目为40）进行病马死亡率的预测。\n",
    "（不使用sklearn 库）"
   ],
   "id": "536fedde191b144b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-06T11:41:22.701467Z",
     "start_time": "2024-11-06T11:41:11.667808Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from jedi.api.helpers import infer\n",
    "from matplotlib.pyplot import figure\n",
    "from mpmath import matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):\n",
    "    retArray=np.ones((np.shape(dataMatrix)[0],1))\n",
    "    if threshIneq=='lt':\n",
    "        retArray[dataMatrix[:,dimen]<=threshVal]=-1.0\n",
    "    else:\n",
    "        retArray[dataMatrix[:,dimen]>threshVal]=-1.0\n",
    "    return retArray\n",
    "\n",
    "def buildStump(dataArr,classLabels,D):\n",
    "    dataMatrix=np.mat(dataArr)\n",
    "    labelMat=np.mat(classLabels).T\n",
    "    m,n=np.shape(dataMatrix)\n",
    "    numSteps=10.0\n",
    "    bestStump={}\n",
    "    bestClasEst=np.mat(np.zeros((m,1)))\n",
    "    minError=np.inf\n",
    "    for i in range(n):\n",
    "        rangeMin=dataMatrix[:,i].min()\n",
    "        rangeMax=dataMatrix[:,i].max()\n",
    "        stepSize=(rangeMax-rangeMin)/numSteps\n",
    "        for j in range(-1,int(numSteps)+1):\n",
    "            for inequal in ['lt','gt']:\n",
    "                threshVal=(rangeMin+float(j)*stepSize)\n",
    "                predictedVals=stumpClassify(dataMatrix,i,threshVal,inequal)\n",
    "                errArr=np.mat(np.ones((m,1)))\n",
    "                errArr[predictedVals==labelMat]=0\n",
    "                weightedError=D.T*errArr\n",
    "                # print(\"split: dim %d, thresh %.2f, thresh inequal: %s, the weighted error is %.3f\"%(i,threshVal,inequal,weightedError))\n",
    "                if weightedError<minError:\n",
    "                    minError=weightedError\n",
    "                    bestClasEst=predictedVals.copy()\n",
    "                    bestStump['dim']=i\n",
    "                    bestStump['thresh']=threshVal\n",
    "                    bestStump['ineq']=inequal\n",
    "    return bestStump,minError,bestClasEst\n",
    "\n",
    "def adaBoostTrainDS(dataArr,classLabels,numIt=40):\n",
    "    weakClassArr=[]\n",
    "    m=np.shape(dataArr)[0]\n",
    "    D=np.mat(np.ones((m,1))/m)\n",
    "    aggClassEst=np.mat(np.zeros((m,1)))\n",
    "    for i in range(numIt):\n",
    "        bestStump,error,classEst=buildStump(dataArr,classLabels,D)\n",
    "        # print('D:',D.T)\n",
    "        alpha=float(0.5*np.log((1.0-error)/max(error,1e-16))[0,0])\n",
    "        bestStump['alpha']=alpha\n",
    "        weakClassArr.append(bestStump)\n",
    "        # print('classEst:',classEst.T)\n",
    "        expon=np.multiply(-1*alpha*np.mat(classLabels).T,classEst)\n",
    "        D=np.multiply(D,np.exp(expon))\n",
    "        D=D/D.sum()\n",
    "        aggClassEst+=alpha*classEst\n",
    "\n",
    "        # print('aggClassEst:',aggClassEst.T)\n",
    "        aggErrors=np.multiply(np.sign(aggClassEst)!=np.mat(classLabels).T,np.ones((m,1)))\n",
    "        errorRate=aggErrors.sum()/m\n",
    "        print('total error:',errorRate)\n",
    "        if errorRate==0.0:\n",
    "            break\n",
    "    return weakClassArr\n",
    "\n",
    "def adaClassify(dattoClass,classifierArr):\n",
    "    dataMatrix=np.mat(dattoClass)\n",
    "    m=np.shape(dataMatrix)[0]\n",
    "    aggClassEst=np.mat(np.zeros((m,1)))\n",
    "    for i in range(len(classifierArr)):\n",
    "        classEst=stumpClassify(dataMatrix,classifierArr[i]['dim'],classifierArr[i]['thresh'],classifierArr[i]['ineq'])\n",
    "        aggClassEst+=classifierArr[i]['alpha']*classEst\n",
    "        # print(aggClassEst)\n",
    "    return np.sign(aggClassEst)\n",
    "        \n",
    "def plotROC(presStrengths,classLabels):\n",
    "    cur=(1.0,1.00)\n",
    "    ySum=0.0\n",
    "    numPosClas=sum(np.array(classLabels)==1.0)\n",
    "    yStep=1/float(numPosClas)\n",
    "    xStep=1/float(len(classLabels)-numPosClas)\n",
    "    sortedIndicies=presStrengths.argsort()\n",
    "    fig=plt.figure()\n",
    "    fig.clf()\n",
    "    ax=plt.subplot(111)\n",
    "    for index in sortedIndicies.tolist()[0]:\n",
    "        if classLabels[index]==1.0:\n",
    "            delX=0\n",
    "            delY=yStep\n",
    "        else:\n",
    "            delX=xStep\n",
    "            delY=0\n",
    "            ySum+=cur[1]\n",
    "        ax.plot([cur[0],cur[0]-delX],[cur[1],cur[1]-delY],c='b')\n",
    "        cur=(cur[0]-delX,cur[1]-delY)\n",
    "    ax.plot([0,1],[0,1],'b--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC')\n",
    "    ax.axis([0.1,0.1])\n",
    "    plt.show()\n",
    "    print('AUC:',ySum*xStep)\n",
    "    \n",
    "\n",
    "# datMat=np.matrix([[1.,2.1],[2.,1.1],[1.3,1.],[1.,1.],[2.,1.]])\n",
    "# classLabels=[1.0,1.0,-1.0,-1.0,1.0]\n",
    "# \n",
    "# D=np.mat(np.ones((5,1))/5)\n",
    "# buildStump(datMat,classLabels,D)\n",
    "# \n",
    "# classifierArr=adaBoostTrainDS(datMat,classLabels,9)\n",
    "# \n",
    "# classifierArr=adaClassify([0,0],classifierArr)\n",
    "\n",
    "dataTrain=pd.read_csv('C:/Users/Admin/Desktop/WHU study/programming/python/MachineLearning/exp2/horseColicTraining.txt', sep='\\t', header=None)\n",
    "dataTest=pd.read_csv('C:/Users/Admin/Desktop/WHU study/programming/python/MachineLearning/exp2/horseColicTest.txt', sep='\\t', header=None)\n",
    "dataTrain=np.array(dataTrain)\n",
    "dataTest=np.array(dataTest)\n",
    "\n",
    "for i in range(len(dataTrain[:,-1])):\n",
    "    dataTrain[i,-1]=1 if dataTrain[i,-1]==1 else -1\n",
    "\n",
    "for i in range(len(dataTest[:,-1])):\n",
    "    dataTest[i,-1]=1 if dataTest[i,-1]==1 else -1\n",
    "\n",
    "ss=StandardScaler()\n",
    "dataTrain[:,:-1]=ss.fit_transform(dataTrain[:,:-1])\n",
    "dataTest[:,:-1]=ss.transform(dataTest[:,:-1])\n",
    "\n",
    "classifierArray=adaBoostTrainDS(dataTrain[:,:-1],dataTrain[:,-1],40)\n",
    "prediction=adaClassify(dataTest[:,:-1],classifierArray)\n",
    "\n",
    "errArr=np.mat(np.ones((67,1)))\n",
    "print(\"test error rate:\",errArr[prediction!=(np.mat(dataTest[:,-1]).T)].sum()/len(dataTest[:,-1]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error: 0.2842809364548495\n",
      "total error: 0.2842809364548495\n",
      "total error: 0.24749163879598662\n",
      "total error: 0.24749163879598662\n",
      "total error: 0.25418060200668896\n",
      "total error: 0.2408026755852843\n",
      "total error: 0.2408026755852843\n",
      "total error: 0.22073578595317725\n",
      "total error: 0.24749163879598662\n",
      "total error: 0.23076923076923078\n",
      "total error: 0.2408026755852843\n",
      "total error: 0.2140468227424749\n",
      "total error: 0.22742474916387959\n",
      "total error: 0.21739130434782608\n",
      "total error: 0.22073578595317725\n",
      "total error: 0.21739130434782608\n",
      "total error: 0.22073578595317725\n",
      "total error: 0.22408026755852842\n",
      "total error: 0.23076923076923078\n",
      "total error: 0.2140468227424749\n",
      "total error: 0.22408026755852842\n",
      "total error: 0.21070234113712374\n",
      "total error: 0.20735785953177258\n",
      "total error: 0.20066889632107024\n",
      "total error: 0.20735785953177258\n",
      "total error: 0.2140468227424749\n",
      "total error: 0.20735785953177258\n",
      "total error: 0.2040133779264214\n",
      "total error: 0.21070234113712374\n",
      "total error: 0.2040133779264214\n",
      "total error: 0.20735785953177258\n",
      "total error: 0.21739130434782608\n",
      "total error: 0.1939799331103679\n",
      "total error: 0.1939799331103679\n",
      "total error: 0.20735785953177258\n",
      "total error: 0.20066889632107024\n",
      "total error: 0.19732441471571907\n",
      "total error: 0.19063545150501673\n",
      "total error: 0.20066889632107024\n",
      "total error: 0.19063545150501673\n",
      "total error: 0.19732441471571907\n",
      "total error: 0.18729096989966554\n",
      "total error: 0.20066889632107024\n",
      "total error: 0.18729096989966554\n",
      "total error: 0.2040133779264214\n",
      "total error: 0.19732441471571907\n",
      "total error: 0.20735785953177258\n",
      "total error: 0.1939799331103679\n",
      "total error: 0.20066889632107024\n",
      "total error: 0.19732441471571907\n",
      "total error: 0.2040133779264214\n",
      "total error: 0.19732441471571907\n",
      "total error: 0.1939799331103679\n",
      "total error: 0.1939799331103679\n",
      "total error: 0.1939799331103679\n",
      "total error: 0.1939799331103679\n",
      "total error: 0.19732441471571907\n",
      "total error: 0.19732441471571907\n",
      "total error: 0.1939799331103679\n",
      "total error: 0.19732441471571907\n",
      "total error: 0.18729096989966554\n",
      "total error: 0.19732441471571907\n",
      "total error: 0.18729096989966554\n",
      "total error: 0.18729096989966554\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.18729096989966554\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.18729096989966554\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.18729096989966554\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.18729096989966554\n",
      "total error: 0.18729096989966554\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.16387959866220736\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.16387959866220736\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16387959866220736\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.16387959866220736\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.16387959866220736\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.16387959866220736\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.18729096989966554\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1605351170568562\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1605351170568562\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1605351170568562\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16387959866220736\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.18729096989966554\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.18394648829431437\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.16387959866220736\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.16387959866220736\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1806020066889632\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17725752508361203\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.17391304347826086\n",
      "total error: 0.16387959866220736\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.16387959866220736\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1605351170568562\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1605351170568562\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1605351170568562\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1605351170568562\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1605351170568562\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.16387959866220736\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1705685618729097\n",
      "total error: 0.16722408026755853\n",
      "total error: 0.1705685618729097\n",
      "test error rate: 0.2537313432835821\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
